{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Word Level Tokenize Model**"
      ],
      "metadata": {
        "id": "3HEupeyPsGbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"dataset.csv\"#\"roman-urdu-poetry.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extract poetry text while preserving line breaks\n",
        "text = \"\\n\".join(data[\"Poetry\"].dropna().tolist())\n",
        "print(f\"Dataset length: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPGHT2sRzTVC",
        "outputId": "6f2a83e5-257a-4375-a988-dce807f03a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 700115 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Tokenize text with newlines included === #\n",
        "tokenizer = Tokenizer(filters='')  # Don't filter punctuation or special characters, including newlines\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Convert text to sequences of word indices\n",
        "text_sequences = tokenizer.texts_to_sequences([text])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTjeXjX1zVkW",
        "outputId": "e7bc0550-2708-4adc-8172-7a7ce045d24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 16224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_verse_length = data['Poetry'].dropna().apply(\n",
        "    lambda poem: max([len(verse.strip().split()) for verse in poem.split('\\n')], default=0)\n",
        ").max()\n",
        "\n",
        "print(\"Maximum verse length (in words):\", max_verse_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytq-w6j1z8Zw",
        "outputId": "2c948aa8-2209-494b-a2db-7ed23faba934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum verse length (in words): 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create Input-Target Sequences === #\n",
        "seq_length = max_verse_length\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(seq_length, len(text_sequences)):\n",
        "    inputs.append(text_sequences[i - seq_length:i])\n",
        "    targets.append(text_sequences[i])\n",
        "\n",
        "inputs = np.array(inputs)\n",
        "targets = np.array(targets)"
      ],
      "metadata": {
        "id": "lUcQMn4Tzbt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build the Model === #\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=256, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(1024, return_sequences=False),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "model.build(input_shape=(None, seq_length))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "N74MyrehzeG5",
        "outputId": "82bf4be1-9c34-4148-e579-44e96b9e6eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │       \u001b[38;5;34m4,153,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m5,246,976\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16224\u001b[0m)               │      \u001b[38;5;34m16,629,600\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16224</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,629,600</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,029,920\u001b[0m (99.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,029,920</span> (99.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,029,920\u001b[0m (99.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,029,920</span> (99.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Train Model === #\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 15\n",
        "history = model.fit(inputs, targets, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cjN7n_RzggS",
        "outputId": "33db890e-c75a-4140-f068-210cc3eb0923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.0364 - loss: 7.5656\n",
            "Epoch 2/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 83ms/step - accuracy: 0.0523 - loss: 6.8335\n",
            "Epoch 3/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 84ms/step - accuracy: 0.0828 - loss: 6.4464\n",
            "Epoch 4/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.1155 - loss: 5.9376\n",
            "Epoch 5/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.1524 - loss: 5.2577\n",
            "Epoch 6/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.2166 - loss: 4.3843\n",
            "Epoch 7/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.3595 - loss: 3.3943\n",
            "Epoch 8/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.5128 - loss: 2.5254\n",
            "Epoch 9/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.6492 - loss: 1.8316\n",
            "Epoch 10/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.7687 - loss: 1.2769\n",
            "Epoch 11/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 84ms/step - accuracy: 0.8619 - loss: 0.8503\n",
            "Epoch 12/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.9279 - loss: 0.5336\n",
            "Epoch 13/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.9702 - loss: 0.3135\n",
            "Epoch 14/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.9911 - loss: 0.1682\n",
            "Epoch 15/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.9986 - loss: 0.0782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7V3tF5cOr_MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "model_save_path = \"poetry_model1.h5\"\n",
        "model.save(model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_GF3ymzjox",
        "outputId": "e832edfb-38f2-4718-9345-839d311be47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path1 = \"roman_urdu_poetry_model.keras\"\n",
        "model.save(model_save_path1)"
      ],
      "metadata": {
        "id": "HZnU9QHHjwrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Text Generation with Newline Support ===\n",
        "def generate_text(model, tokenizer, seed_text, num_generate=50, seq_length=10, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0][-seq_length:]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predictions = model.predict(token_list, verbose=0)\n",
        "        predictions = predictions / temperature\n",
        "        #predicted_id = tf.random.categorical(tf.math.log(predictions), num_samples=1).numpy()[0][0]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
        "        # Skip invalid predictions\n",
        "        if predicted_id == 0:\n",
        "            continue\n",
        "\n",
        "        # Append predicted word to the generated text\n",
        "        predicted_word = tokenizer.index_word[predicted_id]\n",
        "\n",
        "\n",
        "        # Append a newline if predicted word contains a newline token\n",
        "        if predicted_word == \"\\n\":\n",
        "            generated_text += \"\\n\"\n",
        "        else:\n",
        "          generated_text += \" \" + predicted_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate poetry with new lines\n",
        "seed_text = \"ishq\"\n",
        "generated_poetry = generate_text(model, tokenizer, seed_text=seed_text, num_generate=50)\n",
        "print(\"\\nGenerated Poetry with New Lines:\\n\", generated_poetry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rndtOIpgkyxM",
        "outputId": "8cc8d615-1c63-4ed6-f779-cf0c3cb9795a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Poetry with New Lines:\n",
            " ishq ke saath \n",
            "tū ne kuchh is se kī sūrat nahīñ \n",
            "is taraf ab tak koī nahīñ hai jahāñ meñ \n",
            "jo dil kī tarah se mitā jaane hai \n",
            "yahī jo thā us ne ki mar jaa.e to achchhā kyā ho \n",
            "ye bhī sach hai ki ulfat meñ pareshāñ na ho \n",
            "ye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"dataset.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "text = \"\\n\".join(data[\"Poetry\"].dropna().tolist())\n",
        "\n",
        "# Tokenize text with newlines included\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Load the trained model\n",
        "model_save_path = \"roman_urdu_poetry_model.keras\"\n",
        "model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "# Determine the maximum verse length\n",
        "max_verse_length = data['Poetry'].dropna().apply(\n",
        "    lambda poem: max([len(verse.strip().split()) for verse in poem.split('\\n')], default=0)\n",
        ").max()\n",
        "seq_length = max_verse_length\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(seed_text, num_generate=50, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(num_generate):\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0][-seq_length:]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
        "        if predicted_id == 0:\n",
        "            continue\n",
        "        predicted_word = tokenizer.index_word.get(predicted_id, '')\n",
        "        if predicted_word == \"\\n\":\n",
        "            generated_text += \"\\n\"\n",
        "        else:\n",
        "            generated_text += \" \" + predicted_word\n",
        "    return generated_text\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Seed Text\", placeholder=\"Enter a seed word for poetry\"),\n",
        "        gr.Slider(minimum=10, maximum=200, step=10, value=50, label=\"Number of Words\"),\n",
        "        gr.Slider(minimum=0.1, maximum=2.0, step=0.1, value=1.0, label=\"Temperature\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Poetry\"),\n",
        "    title=\"Roman Urdu Poetry Generator\",\n",
        "    description=\"Enter a seed word and generate poetry in Roman Urdu with newline support.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "8WAwFpjOqpqA",
        "outputId": "4e098d91-f087-4942-8d23-d64e16071615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://44c56461f117026651.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://44c56461f117026651.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# === Build the Model === #\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=256, input_length=seq_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024, return_sequences=False)),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.build(input_shape=(None, seq_length))\n",
        "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "dgMsx_nHrkbV",
        "outputId": "49e3043f-7c73-41ca-ff1b-3c391d4a924a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │       \u001b[38;5;34m4,153,344\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m10,493,952\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16224\u001b[0m)               │      \u001b[38;5;34m33,242,976\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,344</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">10,493,952</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16224</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">33,242,976</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,890,272\u001b[0m (182.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,890,272</span> (182.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m47,890,272\u001b[0m (182.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,890,272</span> (182.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Train Model === #\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 15\n",
        "history = model2.fit(inputs, targets, batch_size=BATCH_SIZE, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_5h8Vi8sAtk",
        "outputId": "9b117b6d-17bf-4358-8e8e-c156bfba88e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 161ms/step - accuracy: 0.0353 - loss: 7.5623\n",
            "Epoch 2/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 161ms/step - accuracy: 0.0542 - loss: 6.7861\n",
            "Epoch 3/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.0863 - loss: 6.2287\n",
            "Epoch 4/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.1314 - loss: 5.2956\n",
            "Epoch 5/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.2831 - loss: 3.8525\n",
            "Epoch 6/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.5044 - loss: 2.3986\n",
            "Epoch 7/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.7210 - loss: 1.3281\n",
            "Epoch 8/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.8946 - loss: 0.5943\n",
            "Epoch 9/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.9809 - loss: 0.1923\n",
            "Epoch 10/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 162ms/step - accuracy: 0.9993 - loss: 0.0446\n",
            "Epoch 11/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 162ms/step - accuracy: 0.9998 - loss: 0.0134\n",
            "Epoch 12/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0072\n",
            "Epoch 13/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0051\n",
            "Epoch 14/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0039\n",
            "Epoch 15/15\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 162ms/step - accuracy: 0.9999 - loss: 0.0036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path2 = \"roman_urdu_poetry_model_lstm.keras\"\n",
        "model2.save(model_save_path2)"
      ],
      "metadata": {
        "id": "BcUVak4Z4YIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Text Generation with Newline Support ===\n",
        "def generate_text(model, tokenizer, seed_text, num_generate=50, seq_length=10, temperature=1.0):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        # Tokenize the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0][-seq_length:]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predictions = model.predict(token_list, verbose=0)\n",
        "        predictions = predictions / temperature\n",
        "        #predicted_id = tf.random.categorical(tf.math.log(predictions), num_samples=1).numpy()[0][0]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
        "        # Skip invalid predictions\n",
        "        if predicted_id == 0:\n",
        "            continue\n",
        "\n",
        "        # Append predicted word to the generated text\n",
        "        predicted_word = tokenizer.index_word[predicted_id]\n",
        "\n",
        "\n",
        "        # Append a newline if predicted word contains a newline token\n",
        "        if predicted_word == \"\\n\":\n",
        "            generated_text += \"\\n\"\n",
        "        else:\n",
        "          generated_text += \" \" + predicted_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate poetry with new lines\n",
        "seed_text = \"ishq main\"\n",
        "generated_poetry = generate_text(model2, tokenizer, seed_text=seed_text, num_generate=50)\n",
        "print(\"\\nGenerated Poetry with New Lines:\\n\", generated_poetry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjJ0anbcxF7B",
        "outputId": "acdf1294-7fc9-426d-8427-cb629f1475a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Poetry with New Lines:\n",
            " ishq main meñ aur chāñd phuul kī ishq meñ \n",
            "raat ko kyā hai jahāñ ik dariyā se hai tan se jo jaanā hai \n",
            "ishq kī taraf se shīshe meñ jo hai na dil meñ hai mohtasib us ne jo raaz kiyā hai jo raaz jo chhoḍ kar tujhe jaane vaale bhī ham\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}